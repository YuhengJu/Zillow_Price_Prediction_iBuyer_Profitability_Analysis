{"cells":[{"cell_type":"markdown","source":["# Case 1 (Part I): House price prediction\n","\n","In this case (Part I), you will build a multilayer perceptron network to predict the selling price of properties. The dataset consists of all single family houses and condos that were sold in Denver in a given year.\n","\n","You need to submit the following files on canvas site:\n","\n","- A report in the pdf format containing the plots of the training errors for the multi-layer perception model and the linear regression model, and the answers to the two questions below. You should also provide interpretations and implications of each plot/table in your report. It is not enough to simply put a chart or a table of numbers in the report and expect the audience to understand what the chart means and what it implies. The point is to provide some insights for an audience like senior management at Zillow.\n","\n","- The complete Jupyter notebook containing all your Pytorch code with explanations, along with a Markdown text explaining different parts if needed.\n","\n","\n"],"metadata":{"id":"eqM8VQAFpTst"}},{"cell_type":"markdown","source":["---\n","## Kaggle community competition: Prof. X's Prize\n","\n","\n","You need to set up a Kaggle account and joined the Kaggle competition by following the [link](https://www.kaggle.com/t/414a77c12150407d97e39fae245e34ef).\n","\n","- Name your team as Section_X_Team_Y, where X is either A or B or C or D, and Y is your team number.\n","- One of the team members can serve as team leader and invite other members of your team to join the team.\n","\n","- Each team can submit at most 20 predictions daily\n","\n","To get the test error for your model, you need to submit your predicted prices for test data on Kaggle. See Kaggle competition website for more detailed instructions. Note that in Part I of the case, you do not need to worry about optimizing your model to get the lowest error possible. The Part I will be graded based on your implemention of the base models as specified below.  We will come back to optimize the model and compete for Prof. X's Prize in Part II of the case."],"metadata":{"id":"68m8rPx7vka3"}},{"cell_type":"markdown","metadata":{"id":"s4QjqzSng9P7"},"source":["---\n","## Data Loading and Visualize Data\n","\n","\n","The train data and test data are available on the Kaggle competition website.\n","You need to first download them, then upload them to the google colab, and then read the data using pandas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4gWVq036dy1","scrolled":true,"ExecuteTime":{"end_time":"2023-08-14T16:46:12.975303400Z","start_time":"2023-08-14T16:46:12.877462900Z"},"executionInfo":{"status":"error","timestamp":1730938519711,"user_tz":300,"elapsed":325,"user":{"displayName":"Andrea Yu","userId":"07662323666758316845"}},"colab":{"base_uri":"https://localhost:8080/","height":324},"outputId":"706351f1-d0be-4fc1-ce55-dd884c8f453d"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'train.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-93b1e2ec59ae>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[0;31m# Importing pandas, which is a library for data manipulation and analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Read the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"]}],"source":["import pandas as pd  # Importing pandas, which is a library for data manipulation and analysis\n","#Read the datasets\n","train_df =pd.read_csv(\"train.csv\")\n","test_df =pd.read_csv(\"test.csv\")"]},{"cell_type":"markdown","source":["### Visualization of SALE PRICES in train data\n","\n","Let's take a closer look at the sale prices in the train data."],"metadata":{"id":"-Mo-iBoh0MJK"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt  # Importing matplotlib's pyplot for making plots and charts\n","\n","# Set the style\n","sns.set(style=\"whitegrid\")\n","\n","# Create a histogram\n","plt.figure(figsize=(10, 6))\n","sns.histplot(train_df['SALE_PRICE'], bins=50, color='blue')\n","plt.title('Histogram of Sale Prices (Train Data)')\n","plt.xlabel('Sale Price')\n","plt.ylabel('Number of Properties')\n","plt.show()"],"metadata":{"id":"fiGSFJFy0Rl-","colab":{"base_uri":"https://localhost:8080/","height":217},"outputId":"b12cbb58-a049-45c6-e028-8c1c0cde0084","executionInfo":{"status":"error","timestamp":1730938521565,"user_tz":300,"elapsed":1818,"user":{"displayName":"Andrea Yu","userId":"07662323666758316845"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1e19f145469b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create a histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SALE_PRICE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Histogram of Sale Prices (Train Data)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sale Price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","source":["---\n","## Data Preparation\n","\n","The first step when building a neural network model is getting your data into the proper form to feed into the network.\n","\n","- **Train labels**: We need to extract the sale prices from the train data as train labels. Since the house prices can take very large values, to make training fast it is helpful to define the train labels as the sale prices divided by a normalization factor.\n","\n","- **Handing non-numeric features**: Some of the house features are non-numeric. We will learn about how to process categorical data in the upcoming lectures. For now, you can  remove those non-numeric features and only train over the numeric features.\n","\n","- **Feature standardization**: When predicting house prices, you started from features that took a variety of ranges—some features had small floating-point values, and others had fairly large integer values. The model might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice for dealing with such data is to do feature-wise normalization: for each feature in the input data (a column in the input dataframe), we subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has\n","a unit standard deviation. **Note**: We need to ensure that the train and test data go through the same normalization.\n","\n","- **Handling missing values**: There may exist some entries with missing values. After the feature standardization, we can impute the missing values with zeros."],"metadata":{"id":"PHY3Ye4wCmWM"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","#Train labels\n","train_label = train_df['SALE_PRICE']\n","train_df = train_df.drop('SALE_PRICE', axis=1)\n","\n","\n","# Handling non-numeric features\n","numeric_cols_train = train_df.select_dtypes(include=np.number).columns\n","numeric_cols_test = test_df.select_dtypes(include=np.number).columns\n","train_df_numeric = train_df[numeric_cols_train]\n","test_df_numeric = test_df[numeric_cols_test]\n","\n","\n","# Feature standardization\n","def standardize_features(df, mean_values, std_values):\n","    df_standardized = (df - mean_values) / std_values\n","    return df_standardized\n","\n","mean_values_train = train_df_numeric.mean()\n","std_values_train = train_df_numeric.std()\n","\n","mean_values_test = test_df_numeric.mean()\n","std_values_test = test_df_numeric.std()\n","\n","train_df_standardized = standardize_features(train_df_numeric, mean_values_train, std_values_train)\n","test_df_standardized = standardize_features(test_df_numeric, mean_values_test, std_values_test)\n","\n","\n","# Handling missing values\n","train_df_final = train_df_standardized.fillna(0)\n","test_df_final = test_df_standardized.fillna(0)\n"],"metadata":{"id":"WdAGzYA3UAxm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see that the sale_price in train data has a wide range from 50K to 2 million, with the median price 431K. We can divide the sale_price by 100K, so the normalized sale_price is between 0.5 and 20 in training data. Remember, when we output the predicted price for the test data, we need to multiply back the normalization factor."],"metadata":{"id":"rCOi8_E20q0A"}},{"cell_type":"code","source":["#TODO: define labels for train data as the sale prices divided by $100,000\n","normalization_factor = 100000\n","train_labels = train_label / normalization_factor"],"metadata":{"id":"Zp02cVWcBOJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels.shape"],"metadata":{"id":"QjV866fJBuNt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TODO: Write code to construct feature vectors for train and test data after data preparation.\n","train_features = train_df_final\n","test_features = test_df_final"],"metadata":{"id":"ROUIrsA0Bn1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features.shape, test_features.shape"],"metadata":{"id":"lJA07PMIBjPj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we convert features and labels to PyTorch tensors."],"metadata":{"id":"7nclPlDEBtMD"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# Convert training features and labels to PyTorch tensors\n","train_features = torch.tensor(train_features.values.astype(np.float32), dtype=torch.float32)\n","test_features = torch.tensor(test_features.values.astype(np.float32), dtype=torch.float32)\n","train_labels = torch.tensor(train_labels.values.reshape(-1, 1).astype(np.float32), dtype=torch.float32)"],"metadata":{"id":"XcjojyA1r6g1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## DataLoaders and Batching\n","\n","After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n","1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n","2. Create DataLoaders and batch our training, validation, and test Tensor datasets. Note that we will shuffle the train data, so the model will not learn a particular order. For test data, we do not shuffle."],"metadata":{"id":"GpfNC1455HYf"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","#  Create DataLoaders and batch our train data\n","train_data = TensorDataset(train_features, train_labels)\n","train_loader = DataLoader(train_data, batch_size=128, shuffle=True)"],"metadata":{"id":"iFnex-AG5hu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","#TODO: Create DataLoaders and batch for test data\n","test_data = TensorDataset(test_features)\n","test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"],"metadata":{"id":"sL-N-Sr5AGey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a batch to have a sanity check"],"metadata":{"id":"IdAZUijCBb2U"}},{"cell_type":"code","source":["# obtain one batch of training data\n","dataiter = iter(train_loader)\n","features, labels = next(dataiter)\n","\n","print('Sample input size: ', features.size()) # batch_size, seq_length\n","print('Sample input: \\n', features)\n","print()\n","print('Sample label size: ', labels.size()) # batch_size\n","print('Sample label: \\n', labels)"],"metadata":{"id":"qEyW4FHfC-gc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Linear regression as benchmark\n","\n","Let us first build a linear regression model as a benchmark."],"metadata":{"id":"BQ7a1MTxbK_T"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","#TODO: Build a linear regression model network\n","lin_net = nn.Linear(train_features.shape[1], 1)"],"metadata":{"id":"c-OjHSVBEP90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a batch and see the output"],"metadata":{"id":"xkJ_Dr1Y3Vm9"}},{"cell_type":"code","source":["features, labels = next(dataiter)\n","output=lin_net(features)\n","output.shape,labels.shape"],"metadata":{"id":"lDRr20263VEG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"4gP6KXY_TAR3"}},{"cell_type":"markdown","source":["First, we will use GPU training if it is availabe."],"metadata":{"id":"2EBhmGZh4Gt9"}},{"cell_type":"code","source":["#TODO: use GPU for training if it is availabe\n","if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","  print('Training on GPU')\n","else:\n","  device = torch.device('cpu')\n","  print('Training on CPU')\n","\n","lin_net = lin_net.to(device)"],"metadata":{"id":"CFheqQ4Q4L49"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Second, let us specify the loss function."],"metadata":{"id":"SKEzpB5F5FCa"}},{"cell_type":"code","source":["#TODO: specify the loss function for training\n","criterion = nn.MSELoss()"],"metadata":{"id":"_rEH9aVe4_7E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are now ready to train the network.\n","\n","\n","Note that with house prices, as with stock prices, we care about relative quantities more than absolute quantities. Thus we tend to care more about the relative error than about the absolute error. For instance, if our prediction is off by \\\\$100,000 when estimating the sale price of a house which is \\\\$125,000, then we are probably doing a horrible job. On the other hand, if we err by this amount for a house with sale price \\\\$2 million, this might represent a pretty  accurate prediction.\n","\n","To this end, we will use the median error rate (MER) used by [Zestimate](https://www.zillow.com/z/zestimate/) to measure the predictive performance. The error rate is defined as\n","$$\n","\\text{Error Rate} = \\left| \\frac{\\text{Predicted Price}-\\text{Actual Price}}{\\text{Actual Price}} \\right|\n","$$\n","The median error rate is defined as the median of error rates for all properties."],"metadata":{"id":"BFXI-u1qxePM"}},{"cell_type":"code","source":["#TODO: Write code to train the network\n","optimizer = optim.SGD(lin_net.parameters(), lr=0.01)\n","\n","epochs = 100\n","train_losses = []\n","\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  for features, labels in train_loader:\n","      features, labels = features.to(device), labels.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = lin_net(features)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","\n","  train_losses.append(running_loss / len(train_loader))\n","  print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}')"],"metadata":{"id":"kUGatRu_DxzT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the training error (MER) over epochs"],"metadata":{"id":"TpNjElY1D3xa"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","optimizer = optim.SGD(lin_net.parameters(), lr=0.01)\n","\n","epochs = 100\n","train_mers = []\n","\n","for epoch in range(epochs):\n","    epoch_errors = []\n","\n","    for features, labels in train_loader:\n","        features, labels = features.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = lin_net(features)\n","\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        with torch.no_grad():\n","            errors = torch.abs((outputs - labels) / labels) * 100\n","            epoch_errors.extend(errors.cpu().numpy())\n","\n","    epoch_mer = torch.median(torch.tensor(epoch_errors)).item()\n","    train_mers.append(epoch_mer)\n","    print(f'Epoch {epoch+1}, MER: {epoch_mer:.4f}%')\n","\n","\n","import matplotlib.pyplot as plt\n","\n","plt.plot(range(1, epochs + 1), train_mers, label='Train MER')\n","plt.xlabel('Epochs')\n","plt.ylabel('Median Error Rate (%)')\n","plt.legend()\n","plt.title('Training MER over Epochs')\n","plt.show()\n"],"metadata":{"id":"ZvDRqWAbhcsT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Build the Multi-layer Perceptron Base Model"],"metadata":{"id":"c2OHyGi23fKD"}},{"cell_type":"markdown","source":["In the following, we build a multi-layer perception model."],"metadata":{"id":"ucgArogM4SJF"}},{"cell_type":"code","source":["#TODO: Build a multi-layer perception neural network with 2 hidden layers of sizes 256 and 128, respectively and ReLu activations\n","class MLP(nn.Module):\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size1)\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n","        self.relu2 = nn.ReLU()\n","        self.fc3 = nn.Linear(hidden_size2, output_size)\n","\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu1(out)\n","        out = self.fc2(out)\n","        out = self.relu2(out)\n","        out = self.fc3(out)\n","        return out\n","\n","input_size = train_features.shape[1]\n","hidden_size1 = 256\n","hidden_size2 = 128\n","output_size = 1\n","\n","mlp_net = MLP(input_size, hidden_size1, hidden_size2, output_size).to(device)"],"metadata":{"id":"TtyVuY9H3i-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TODO: write code to train the MLP network\n","optimizer = optim.Adam(mlp_net.parameters(), lr=0.001)\n","\n","# Using 100 epochs\n","epochs = 100\n","mlp_train_losses = []\n","\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  for features, labels in train_loader:\n","    features, labels = features.to(device), labels.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    outputs = mlp_net(features)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","\n","    running_loss += loss.item()\n","\n","  mlp_train_losses.append(running_loss / len(train_loader))\n","  print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}')"],"metadata":{"id":"vXSQ6uOx3pLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TODO: Write code to plot the training error (MER) over epochs\n","# Plot\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","optimizer = optim.SGD(mlp_net.parameters(), lr=0.01)\n","\n","epochs = 100\n","train_mers = []\n","\n","for epoch in range(epochs):\n","    epoch_errors = []\n","\n","    for features, labels in train_loader:\n","        features, labels = features.to(device), labels.to(device)\n","\n","        # Zero out the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward\n","        outputs = mlp_net(features)\n","\n","        # Backpropagation loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backward + optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Error percentage\n","        with torch.no_grad():\n","            errors = torch.abs((outputs - labels) / labels) * 100\n","            epoch_errors.extend(errors.cpu().numpy())\n","\n","    # MER\n","    epoch_mer = torch.median(torch.tensor(epoch_errors)).item()\n","    train_mers.append(epoch_mer)\n","    print(f'Epoch {epoch+1}, MER: {epoch_mer:.4f}%')\n","\n","\n","# Plot\n","import matplotlib.pyplot as plt\n","\n","plt.plot(range(1, epochs + 1), train_mers, label='Train MER')\n","plt.xlabel('Epochs')\n","plt.ylabel('Median Error Rate (%)')\n","plt.legend()\n","plt.title('Training MER over Epochs')\n","plt.show()"],"metadata":{"id":"RBr4bAXe3ngp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question 1**: What are your final training errors of the multilayer perception model and the linear regression model?"],"metadata":{"id":"t9e9Yeqr8PZZ"}},{"cell_type":"markdown","source":["At epoch 100, the median error rate for the linear model is 22.0745% and the median error rate for the multilayer perceptron model is 13.0823%."],"metadata":{"id":"CZKEy4QWoG80"}},{"cell_type":"markdown","source":["---\n","## Inference on test data\n","\n","After the MLP model is trained, we can use it for inference."],"metadata":{"id":"08gC_-Q4pDon"}},{"cell_type":"code","source":["#TODO: write the code to generate predicted sale prices for test data. using mlp model\n","\n","predicted_prices = []\n","mlp_net.eval()\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs = data[0].to(device)\n","        outputs = mlp_net(inputs)\n","        predicted_prices.extend(outputs.cpu().numpy())\n","\n","predicted_prices = np.array(predicted_prices).flatten() * normalization_factor"],"metadata":{"id":"z7x7bScqiww2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TODO: save the predicted sale prices into submission_csv\n","import pandas as pd\n","submission_df = pd.read_csv('sample_submission.csv')"],"metadata":{"id":"KmN6bTDupJyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_df['SALE_PRICE'] = predicted_prices"],"metadata":{"id":"3JmJ6d_Xn7fK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_df.to_csv('submission.csv', index=False)"],"metadata":{"id":"yzzVbPe-oc1Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can submit our predictions on Kaggle and see how they compare with the actual house prices (labels) on the test set.\n","\n","- Log in to the Kaggle website and visit the house price prediction competition page.\n","\n","- Click the “Submit Predictions”.\n","\n","- Click the “Browse Files” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n","\n","- Click the “Submit” button at the bottom of the page to view your results."],"metadata":{"id":"gNje9TjRphTx"}},{"cell_type":"markdown","source":["**Question 2**: What is the test error shown on Kaggle? How does it compare with the train error?"],"metadata":{"id":"28-wqckhrA0z"}},{"cell_type":"markdown","source":["The training error in Kaggle is 0.14566. It is a bit higher than the training error we got which was 13.0823%."],"metadata":{"id":"Dmyl8qHqqJRs"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["s4QjqzSng9P7","PHY3Ye4wCmWM","GpfNC1455HYf","BQ7a1MTxbK_T","4gP6KXY_TAR3","c2OHyGi23fKD","08gC_-Q4pDon"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}